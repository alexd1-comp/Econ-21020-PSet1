%%%%%%%%%%%%%%%%%%%%%%
% Preamble %%%%%%%%%%%
\input{preamble.tex}

%%%%%%%%%%%%%%%%%%%%%%
% Main Document %%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%
% Document header %%%%
{\LARGE \centering ECON 21020 -- Problem Set 1\par}
{\vspace{-1em} \large \centering Alex Ding \par}
{\centering \vspace{-1em} \today \par }

%%%%%%%%%%%%%%%%%%%%%%
% Problem 1 %%%%%%%%%%
\section{}
Table 1 gives the joint probability mass function between employment status and college degree in the US working-age population.
\begin{center}
Table 1: Joint Probability Mass Function
\end{center}

\begin{table}[h!]
\centering
 \begin{tabular}{||c c c||} 
 \hline
 & Unemployed (Y = 0) & Employed (Y = 1)  \\ [0.5ex] 
 \hline\hline
 Non-college grads (X = 0) & 0.026 & 0.576 \\ 
 College grads (X = 1) & 0.009 & 0.389 \\ [1ex] 
 \hline
 \end{tabular}
\end{table}
 
 $\textbf{1.1}$ An economic interpretation of the statement P(Y = 1, X = 1) = 0.389 is that out of the sample space, the working-age population in the U.S, 38.9$\%$ of the working-age population are both a college grad and are employed.
 
 $\textbf{1.2}$ The unconditional probability of being employed P(Y = 1) is 0.576 + 0.389 = 0.965.
 
$\textbf{1.3}$ The unconditional probability of having a college degree P(X = 1) is 0.009 + 0.389 = 0.398.
 
$\textbf{1.4}$ The unemployment share for college graduates is P(Y = 0 $\mid$ X = 1). 
 This is equivalent to P(X = 1 $\cap$ Y = 0) / P(X). Using the table, this is (0.009)/(0.398).
 
 The final answer is 0.009/0.398 = 0.0226.
 
 The unemployment share for non-college graduates is P(Y = 0 $\mid$ X = 0). This is equivalent to P(Y = 0 $\cap$ X = 0) / P(X). 
 Using the table this is (0.026)/(0.602).

 The final answer is 0.026/0.602 = 0.0432.
 
$\textbf{1.5}$ Employment status and college degree are not independent, as P(X = 0) $\neq$ P(X = 0 $\cap$ Y = 0).
 Similarly, P(X = 1) $\neq$ P(X = 1 $\cap$ Y = 0).
 
 We got P(X = 0) = 0.602 while P (X = 0$\cap$ Y = 0) is equal to 0.026/0.035 = 0.7428. 
 
 0.602 $\neq$ 0.7428.
 
%%%%%%%%%%%%%%%%%%%%%%
% Problem 2 %%%%%%%%%%
\newpage
\section{}

Let X $\sim$N(0,1) and define Y $\equiv$ a + bX for a,b $\in$ R.

$\textbf{2.1}$ E[Y] = $E_X$[h(X)], by the Law of the Unconscious Statistician.

Thus, E[Y] = $E_X$[h(X)] = \int_{-\infty}^\infty h(x)f$_X$(x)dx,
as X is continuous.

It is known that  Y $\equiv$ a + bX, so E[a + bX] = \int_{-\infty}^\infty a + bX f$_X$(x)dx

Since X is the standard normal distribution, we know that f(x)= e^{-x^2/2}/\sqrt{2\pi}

Plugging that into the prior integral, we get that E[Y] = E[a+bX] = \int_{0}^1 a + bx * e^{-x^2/2}/\sqrt{2\pi}dx

$\textbf{2.2}$  We know that Var(a+bX) = b^2Var(X). 

Using that equation, we get Var(Y) = Var(a+bX) = $b^2Var(X)$ = $b^2$. 

Since X is the standard normal distribution, Var(X) = 1.

%%%%%%%%%%%%%%%%%%%%%%
% Problem 3 %%%%%%%%%%
\newpage
\section{}

Take n $\in$ N and let {\{$X_i}\}_{i=1}}^n$ be a collection of independent random variables with supp X_i = $$\{0,1\}$ and p = $P(X_i = 1)$,  $\forall$i = 1,....,n.
Define $X_n$ = {\{$X_i}\}_{i=1}}^n$ $X_i$.

$\textbf{3.1}$ $E[X_n]$ = E($\sum_{i=1}^n{X_i}$)

=  $\sum_{i=1}^nE({X_i})$ by Sum of expectations of independent trials, meaning the expectation of a product of independent random variable is the product of their expectations.

We know that p = $P(X_i = 1)$ = $E(X_i)$ due  to $X_i$ being a random variable in the Bernoulli(p) distribution.

Thus, the sum of n identical $X_i$ terms is equivalent to the product of n * the expectation of $X_i$. Therefore, $E[X_n]$ = n*p = np.

$\textbf{3.2}$ We know that the expectation of a binomial distribution $E(X_n)$ = np from 3a.

Var(X) = $E(X^2)$ = $(E(X))^2$

$E(X^2)$ = np- $np^2$ = np(1-p)

$(E(X))^2$ = $n^2p^2$ - $(np)^2$ = 0

Thus Var(X) = np(1-p)


%%%%%%%%%%%%%%%%%%%%%%
% Problem 4 %%%%%%%%%%
\newpage
\section{}

$\textbf{4.1}$ Show that P(Y=y) = $\sum_{x\in g^-1(y)}$ P(X=x).

E[Y] = $\sum_{y\in Y}$y P(Y=y), so by the definition of expected value of a discrete variable, 

$f_Y(y)$ = P(Y=y).

We know that Y $\equiv$ g(x), 

so $g^{-1}$ (y) = $\{x_1,x_2,....\}$ when x $\in$ $\mathcal{X}$

So that all $x_i$ sum to $\sum_{x\in g^-1(y)}$ P(X = $x_i$)

Finally, $f_y(Y)$ = P(Y=y) = $\sum_{x\in g^-1(y)}$ P(X = $x_i$).


$\textbf{4.2}$ Show that E[g(X)] = $\sum_{x\in \mathcal{X}}$ g(x) P(X = x)

E[Y] = E[g(X)] = $\sum_{y\in Y}$y P(Y=y).

From part 4a), we know that P(Y=y) = $\sum_{x\in g^-1(y)}$ P(X=x), so therefore E[g(X)] = $\sum_{y\in Y}$ y $\sum_{x\in g^-1(y)}$ P(X=x). 

Furthermore, y = g(x), so  E[g(X)] = $\sum_{y\in Y}$ g(x) $\sum_{x\in g^-1(y)}$ P(X=x) = E[g(X)] 

= $\sum_{y\in Y}$ $\sum_{x\in g^-1(y)}$  g(x) P(X=x).

From the hint provided, it is known that $\sum_{y\in Y}$ $\sum_{x\in g^-1(y)}$ could be better written as $\sum_{x\in \mathcal{X}}$.

Therefore, the final equation is E[g(X)] = $\sum_{x\in \mathcal{X}}$ g(x) P(X = x).

%%%%%%%%%%%%%%%%%%%%%%
% Problem 5 %%%%%%%%%%
\newpage
\section{}

$\textbf{5.1}$ Show that $\underset{a \in{R}}{\arg\min}$ E[(X-a)^2] = E[X]. 

That is, show that the mean of X is the best constant predictor of X under the $L^2$ loss function.

Take the first order condition of \underset{a \in{R}}{\arg\min}$ E[(X-a)^2].

2a - 2E[X]a + 2E[X] = 0

2E[X](a+1) = -2a

2E[X] = -2a/a+1

E[X] = -a/2(a+1)

which is minimized when a = E[X].

%%%%%%%%%%%%%%%%%%%%%%
% Problem 6 %%%%%%%%%%
\newpage
\section{}

$\textbf{6.1}$ Define U $\equiv$ X - E[X$\mid$Y]. Show that E[U$\mid$Y] = 0.

E[U$\mid$Y] = E[X-E[X$\mid$Y]$\mid$Y] 

= E[X$\mid$ Y - E[X$\mid$Y]]

= E[X $\mid$ Y] -E[E[X$\mid$ Y]]

= E[X $\mid$ Y] - E[X $\mid$ Y] = 0

Thus, E[U$\mid$Y] = 0

$\textbf{6.2}$ Let h be some function of Y such that E[h(Y)$\mid$] $< \infty$.

Show that E[Uh(Y)] = 0

E[Uh(Y)] = E[E[U$\mid$Y]h(Y)] 

= E[h(Y)] - E[U$\mid$Y]]

= As E[U$\mid$Y] = 0 because of 6a.

= E[h(Y)] - E[U$\mid$ Y] = 0.

Thus, E[Uh(Y)] = 0.

$\textbf{6.3}$ Show that E[V^2] $\geq$ E[U^2].

[V]^2 = [U - h(Y)]^2

E[V]^2 = E[U - h(Y)]^2

E[V]^2 = E[U]^2 - 2E[Uh(Y)] + h(Y)^2

2E[Uh(Y)] = 0 from 6b.

$E[V]^2 = E[U]^2 + h(Y)^2.$ As h(Y)^2 > 0, 

Therefore, E[V^2] $\geq$ E[U^2].

$\textbf{6.4}$ The result from part (c) shows that E[X$\mid$ Y] is the best predictor of X given Y under the $L^2$ loss. Because of the result from part (c), it is shown that there cannot be any predictor that is better due to the inequality.

%%%%%%%%%%%%%%%%%%%%%%
% Problem 7 %%%%%%%%%%
\newpage
\section{}

$\textbf{7.1}$

Show that if X is independent to Y, then E[Y $\mid$ X] = E[Y].

Let A $\in$ X so $\int_{A}$ YdP = Y * E[I_{A}] = E[Y]E[I_{A}] = E[Y]P(A) = $\int_{A}$E[Y]dP. 

Therefore, by the definition of conditional expectation, E[Y] = E[Y $\mid$ X].

$\textbf{7.2}$ If X is independent to Y, E[Y $\mid$ X] = E[Y] and E[X $\mid $Y] = E[X]. 
As X is independent to Y, E[XY] = E[X]E[Y]. 

Cov(X,Y) = E[(X- $\mu$x)]E([Y]-$\mu$y) = E[XY] -E[X]E[Y].

Since E[XY] = E[X]E[Y], E[XY] - E[X]E[Y] = 0.

As Cov(X,Y) = 0, thus Corr(X,Y) = 0 also.

$\textbf{7.3}$ Let's say (X,Y) is drawn uniformly at random from four points (0,1), (0,-1), (1,0), and (-1,0).

We know that E(X) = 0, since 1/2 * 0 + 1/4 * 1 + 1/4 *-1 = 0.

Similarly, E(Y) = 0.

E(XY) = 0, because XY = 0 almost surely

Therefore, Cov(X,Y) = E(XY) = 0, so Corr(X,Y) = 0.

However, X is not independent to Y since when X = -1, Y has to = 0, while when X = 0, Y can equal -1 $\leq$ Y $\leq$ 1.

%%%%%%%%%%%%%%%%%%%%%%
% Problem 8 %%%%%%%%%%
\newpage
\section{}

$\textbf{8.1}$ Generate n = 10000 draws from a standard normal random variable N(0,1) and plot the simulated data in a histogram using ggplot2.

\begin{lstlisting}[language=R]
library(ggplot2)
data=data.frame(value=rnorm(10000))
p <- ggplot(data, aes(x=value)) + 
geom_histogram()
p
\end{lstlisting}


$\textbf{8.2}$
\begin{lstlisting}[language=R]
y = runif(10000,-1,1)
data=data.frame(value=y)
p <- ggplot(data, aes(x=value)) + 
geom_histogram()
p
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%
% Problem 9 %%%%%%%%%%
\newpage
\section{}

$\textbf{9.1}$ Let X $\sim$ Bernoulli(p) and U $\sim$ Uniform(0,1). Show that

P(I{\{U $\leq$ p}\} = 1) = p

and conclude that I\{U $\leq$p\} and X are identically distributed. 

P(I{\{U $\leq$ p}\} = 1) = P(U $\leq$ p)

= p - 0 / 1- 0  = p. Comes from the distribution function of uniform random variables.

Therefore, as P(I{\{U $\leq$ p}\} = 1) = P(X=1), and supp I\{U$\leq$ p\}, = supp X = \{0,1\}, it follows that U and X are identically distributed.

$\textbf{9.2}$

\begin{lstlisting}[language=R]
my_rbernoulli <- function (n,p){
  runif(n) < p
}
x <- my_rbernoulli(10000,0.5)
length(x) == 10000
mean(x)
\end{lstlisting}

$\textbf{9.3}$

\begin{lstlisting}[language=R]
my_rbinomial <- function(n,p,m){
  m * my_rbernoulli(n,p)
}
x <- my_rbinomial(10000,0.5,10)
length(x) == 10000
mean(x)
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% Github Link to Repository %%%%%%%%%
\newpage
\textbf{Github Link to Repository:}

https://github.com/alexd1-comp/Econ-21020-PSets




\end{document}