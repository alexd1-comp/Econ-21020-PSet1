%%%%%%%%%%%%%%%%%%%%%%
% Preamble %%%%%%%%%%%
\input{preamble.tex}

%%%%%%%%%%%%%%%%%%%%%%
% Main Document %%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%
% Document header %%%%
{\LARGE \centering ECON 21020 -- Problem Set 1\par}
{\vspace{-1em} \large \centering Alex Ding \par}
{\centering \vspace{-1em} \today \par }

%%%%%%%%%%%%%%%%%%%%%%
% Problem 1 %%%%%%%%%%
\section{}
Table 1 gives the joint probability mass function between employment status and college degree in the US working-age population.
\begin{center}
Table 1: Joint Probability Mass Function
\end{center}

\begin{table}[h!]
\centering
 \begin{tabular}{||c c c||} 
 \hline
 & Unemployed (Y = 0) & Employed (Y = 1)  \\ [0.5ex] 
 \hline\hline
 Non-college grads (X = 0) & 0.026 & 0.576 \\ 
 College grads (X = 1) & 0.009 & 0.389 \\ [1ex] 
 \hline
 \end{tabular}
\end{table}
 a) An economic interpretation of the statement P(Y = 1, X = 1) = 0.389 is that 0.389 of the working-age population in the U.S is both a college grad and employed.
 
 b) The unconditional probability of being employed P(Y = 1) is 0.576 + 0.389 = 0.965.
 
 c) The unconditional probability of having a college degree P(X = 1) is 0.009 + 0.389 = 0.398.
 
 d) The unemployment share for college graduates is P(Y = 0 $\mid$ X = 1). 
 This is equivalent to P(X = 1 $\cap$ Y = 0) / P(X). Using the table, this is (0.009)/(0.398).
 
 The final answer is 0.009/0.398 = 0.0226.
 
 The unemployment share for non-college graduates is P(Y = 0 $\mid$ X = 0). This is equivalent to P(Y = 0 $\cap$ X = 0) / P(X). 
 Using the table this is (0.026)/(0.602).

 The final answer is 0.026/0.602 = 0.0432.
 
 e) Employment status and college degree are not independent, as P(X = 0) $\neq$ P(X = 0 $\cap$ Y = 0).
 Similarly, P(X = 1) $\neq$ P(X = 1 $\cap$ Y = 0).
 
 We got P(X = 0) = 0.602 while P (X = 0$\cap$ Y = 0) is equal to 0.026/0.035 = 0.7428. 
 
 0.602 $\neq$ 0.7428.
 
%%%%%%%%%%%%%%%%%%%%%%
% Problem 2 %%%%%%%%%%
\newpage
\section{}

Let X $\sim$N(0,1) and define Y $\equiv$ a + bX for a,b $\in$ R.

2a) E[Y] = $E_X$[h(X)], by the Law of the Unconscious Statistician.

Thus, E[Y] = $E_X$[h(X)] = \int_{-\infty}^\infty h(x)f$_X$(x)dx,
as X is continuous.

It is known that  Y $\equiv$ a + bX, so E[a + bX] = \int_{-\infty}^\infty a + bX f$_X$(x)dx

Since X is the standard normal distribution, we know that f(x)= e^{-x^2/2}/\sqrt{2\pi}

Plugging that into the prior integral, we get that E[Y] = E[a+bX] = \int_{0}^1 a + bx * e^{-x^2/2}/\sqrt{2\pi}dx

2b)

We know that Var(a+bX) = b^2Var(X), so:

Var(Y) = Var(a+bX) = $b^2Var(X)$ = $b^2$, since X is the standard normal distribution, Var(X) = 1.

%%%%%%%%%%%%%%%%%%%%%%
% Problem 3 %%%%%%%%%%
\newpage
\section{}

Take n $\in$ N and let {\{$X_i}\}_{i=1}}^n$ be a collection of independent random variables with supp X_i = $$\{0,1\}$ and p = $P(X_i = 1)$,  $\forall$i = 1,....,n.
Define $X_n$ = {\{$X_i}\}_{i=1}}^n$ $X_i$.

3a) $E[X_n]$ = E($\sum_{i=1}^n{X_i}$)

=  $\sum_{i=1}^nE({X_i})$ by Sum of expectations of independent trials, meaning the expectation of a product of independent random variable is the product of their expectations.

We know that p = $P(X_i = 1)$ = $E(X_i)$ due  to $X_i$ being a random variable in the Bernoulli(p) distribution.

Thus, the sum of n identical $X_i$ terms is equivalent to the product of n * the expectation of $X_i$. Therefore, $E[X_n]$ = n*p = np.

3b)

We know that the expectation of a binomial distribution $E(X_n)$ = np from 3a.

Var(X) = $E(X^2)$ = $(E(X))^2$

$E(X^2)$ = np- $np^2$ = np(1-p)

$(E(X))^2$ = $n^2p^2$ - $(np)^2$ = 0

Thus Var(X) = np(1-p)


%%%%%%%%%%%%%%%%%%%%%%
% Problem 4 %%%%%%%%%%
\newpage
\section{}

4a) Show that P(Y=y) = $\sum_{x\in g^-1(y)}$ P(X=x).

E[Y] = $\sum_{y\in Y}$y P(Y=y), so by the definition of expected value of a discrete variable, 

$f_Y(y)$ = P(Y=y).

We know that Y $\equiv$ g(x), 

so $g^{-1}$ (y) = $\{x_1,x_2,....\}$ when x $\in$ $\mathcal{X}$

So that all $x_i$ sum to $\sum_{x\in g^-1(y)}$ P(X = $x_i$)

Finally, $f_y(Y)$ = P(Y=y) = $\sum_{x\in g^-1(y)}$ P(X = $x_i$).


4b) Show that E[g(X)] = $\sum_{x\in \mathcal{X}}$ g(x) P(X = x)

E[Y] = E[g(X)] = $\sum_{y\in Y}$y P(Y=y).

From part 4a), we know that P(Y=y) = $\sum_{x\in g^-1(y)}$ P(X=x), so therefore E[g(X)] = $\sum_{y\in Y}$ y $\sum_{x\in g^-1(y)}$ P(X=x). 

Furthermore, y = g(x), so  E[g(X)] = $\sum_{y\in Y}$ g(x) $\sum_{x\in g^-1(y)}$ P(X=x) = E[g(X)] 

= $\sum_{y\in Y}$ $\sum_{x\in g^-1(y)}$  g(x) P(X=x).

From the hint provided, it is known that $\sum_{y\in Y}$ $\sum_{x\in g^-1(y)}$ could be better written as $\sum_{x\in \mathcal{X}}$.

Therefore, the final equation is E[g(X)] = $\sum_{x\in \mathcal{X}}$ g(x) P(X = x).

%%%%%%%%%%%%%%%%%%%%%%
% Problem 5 %%%%%%%%%%
\newpage
\section{}

5a) Show that $\underset{a \in{R}}{\arg\min}$ E[(X-a)^2] = E[X]. 

That is, show that the mean of X is the best constant predictor of X under the $L^2$ loss function.

Take the first order condition of \underset{a \in{R}}{\arg\min}$ E[(X-a)^2].

2a - 2EXa + 2EX = 0

2EX(a+1) = -2a

2EX = -2a/a+1

EX = -a/2(a+1)

which is minimized when a = E[X].

%%%%%%%%%%%%%%%%%%%%%%
% Problem 6 %%%%%%%%%%
\newpage
\section{}

6a) Define U $\equiv$ X - E[X$\cap$Y]. Show that E[U$\cap$Y] = 0.



%%%%%%%%%%%%%%%%%%%%%%
% Github Link to Repository %%%%%%%%%
\newpage
\textbf{Github Link to Repository:}

https://github.com/alexd1-comp/Econ-21020-PSets

%%%%%%%%%%%%%%%%%%%%%%
% Appendix %%%%%%%%%%%
\newpage
\appendix

\section{Code}

This appendix contains the code used for the analysis. 

\begin{lstlisting}[language=R, basicstyle=\footnotesize\ttfamily]
# Insert code here (if applicable).
x <- FALSE
if (x) {
    make_a_joke()
}#IF
\end{lstlisting}



\end{document}